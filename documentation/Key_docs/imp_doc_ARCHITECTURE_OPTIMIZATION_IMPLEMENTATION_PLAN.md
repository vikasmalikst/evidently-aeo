# Architecture Optimization Implementation Plan
## End-to-End Migration Strategy with Zero Downtime

---

## üéØ Executive Summary

**Goal**: Migrate from current `extracted_positions` monolithic table to optimized star schema

**Benefits**:
- 84% storage reduction (1,840MB ‚Üí 294MB)
- 90x faster dashboard queries (450ms ‚Üí 5ms)
- 6x faster writes (60K rows ‚Üí 10K rows)
- Better scalability and maintainability

**Timeline**: 2-3 weeks (4 phases)

**Downtime**: **ZERO** - System remains operational throughout

**Risk Level**: üü¢ **LOW** (dual-write strategy, rollback plans, extensive testing)

---

## üìã Migration Strategy Overview

```
Phase 1: CREATE new schema (no impact)
  ‚Üì
Phase 2: BACKFILL data (runs in background)
  ‚Üì
Phase 3: DUAL-WRITE (system writes to both old + new tables)
  ‚Üì
Phase 4: MIGRATE queries (gradual, service by service)
  ‚Üì
Phase 5: DEPRECATE old table (cleanup)
```

**Key Principle**: Old system keeps running while new system is built in parallel

---

## üóìÔ∏è Detailed Phase Breakdown

### Phase 1: Create New Schema (Day 1-2)
**Duration**: 2-4 hours  
**Downtime**: ZERO  
**Risk**: üü¢ NONE (only creates tables, doesn't touch existing data)

#### 1.1 Create Core Tables

```sql
-- Step 1.1.1: Create metric_facts table
CREATE TABLE metric_facts (
  id BIGSERIAL PRIMARY KEY,
  collector_result_id BIGINT NOT NULL UNIQUE REFERENCES collector_results(id) ON DELETE CASCADE,
  brand_id UUID NOT NULL REFERENCES brands(id) ON DELETE CASCADE,
  customer_id UUID NOT NULL REFERENCES customers(id),
  query_id UUID NOT NULL REFERENCES generated_queries(id),
  collector_type TEXT NOT NULL,
  topic TEXT,
  processed_at TIMESTAMPTZ NOT NULL,
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Indexes for metric_facts
CREATE INDEX idx_metric_facts_collector_result ON metric_facts(collector_result_id);
CREATE INDEX idx_metric_facts_brand_processed ON metric_facts(brand_id, processed_at DESC);
CREATE INDEX idx_metric_facts_brand_collector ON metric_facts(brand_id, collector_type, processed_at DESC);
CREATE INDEX idx_metric_facts_topic ON metric_facts(topic) WHERE topic IS NOT NULL;

-- Step 1.1.2: Create brand_metrics table
CREATE TABLE brand_metrics (
  id BIGSERIAL PRIMARY KEY,
  metric_fact_id BIGINT NOT NULL UNIQUE REFERENCES metric_facts(id) ON DELETE CASCADE,
  visibility_index NUMERIC(5,2),
  share_of_answers NUMERIC(5,2),
  has_brand_presence BOOLEAN NOT NULL DEFAULT false,
  brand_first_position INTEGER,
  brand_positions INTEGER[],
  total_brand_mentions INTEGER NOT NULL DEFAULT 0,
  total_word_count INTEGER NOT NULL DEFAULT 0
);

CREATE INDEX idx_brand_metrics_fact ON brand_metrics(metric_fact_id);
CREATE INDEX idx_brand_metrics_visibility ON brand_metrics(visibility_index) WHERE visibility_index IS NOT NULL;

-- Step 1.1.3: Create competitor_metrics table
CREATE TABLE competitor_metrics (
  id BIGSERIAL PRIMARY KEY,
  metric_fact_id BIGINT NOT NULL REFERENCES metric_facts(id) ON DELETE CASCADE,
  competitor_id INTEGER NOT NULL REFERENCES brand_competitors(id) ON DELETE CASCADE,
  visibility_index NUMERIC(5,2),
  share_of_answers NUMERIC(5,2),
  competitor_positions INTEGER[],
  competitor_mentions INTEGER NOT NULL DEFAULT 0,
  UNIQUE(metric_fact_id, competitor_id)
);

CREATE INDEX idx_competitor_metrics_fact_comp ON competitor_metrics(metric_fact_id, competitor_id);
CREATE INDEX idx_competitor_metrics_comp ON competitor_metrics(competitor_id);
CREATE INDEX idx_competitor_metrics_visibility ON competitor_metrics(competitor_id, visibility_index) WHERE visibility_index IS NOT NULL;

-- Step 1.1.4: Create brand_sentiment table
CREATE TABLE brand_sentiment (
  id BIGSERIAL PRIMARY KEY,
  metric_fact_id BIGINT NOT NULL UNIQUE REFERENCES metric_facts(id) ON DELETE CASCADE,
  sentiment_label TEXT CHECK (sentiment_label IN ('POSITIVE', 'NEGATIVE', 'NEUTRAL')),
  sentiment_score NUMERIC(5,2),
  positive_sentences JSONB DEFAULT '[]'::jsonb,
  negative_sentences JSONB DEFAULT '[]'::jsonb
);

CREATE INDEX idx_brand_sentiment_fact ON brand_sentiment(metric_fact_id);
CREATE INDEX idx_brand_sentiment_label ON brand_sentiment(sentiment_label) WHERE sentiment_label IS NOT NULL;

-- Step 1.1.5: Create competitor_sentiment table
CREATE TABLE competitor_sentiment (
  id BIGSERIAL PRIMARY KEY,
  metric_fact_id BIGINT NOT NULL REFERENCES metric_facts(id) ON DELETE CASCADE,
  competitor_id INTEGER NOT NULL REFERENCES brand_competitors(id) ON DELETE CASCADE,
  sentiment_label TEXT CHECK (sentiment_label IN ('POSITIVE', 'NEGATIVE', 'NEUTRAL')),
  sentiment_score NUMERIC(5,2),
  positive_sentences JSONB DEFAULT '[]'::jsonb,
  negative_sentences JSONB DEFAULT '[]'::jsonb,
  UNIQUE(metric_fact_id, competitor_id)
);

CREATE INDEX idx_competitor_sentiment_fact_comp ON competitor_sentiment(metric_fact_id, competitor_id);
CREATE INDEX idx_competitor_sentiment_comp ON competitor_sentiment(competitor_id);
```

#### 1.2 Create Materialized View

```sql
-- Step 1.2: Create daily aggregated metrics view
CREATE MATERIALIZED VIEW mv_brand_daily_metrics AS
SELECT 
  mf.brand_id,
  mf.customer_id,
  mf.collector_type,
  DATE(mf.processed_at) as metric_date,
  -- Visibility metrics
  AVG(bm.visibility_index) as avg_visibility,
  AVG(bm.share_of_answers) as avg_share,
  COUNT(*) FILTER (WHERE bm.has_brand_presence) as presence_count,
  COUNT(*) as total_responses,
  -- Sentiment metrics
  AVG(bs.sentiment_score) as avg_sentiment,
  COUNT(*) FILTER (WHERE bs.sentiment_label = 'POSITIVE') as positive_count,
  COUNT(*) FILTER (WHERE bs.sentiment_label = 'NEGATIVE') as negative_count,
  COUNT(*) FILTER (WHERE bs.sentiment_label = 'NEUTRAL') as neutral_count,
  -- Timestamps
  MIN(mf.processed_at) as first_response,
  MAX(mf.processed_at) as last_response
FROM metric_facts mf
LEFT JOIN brand_metrics bm ON bm.metric_fact_id = mf.id
LEFT JOIN brand_sentiment bs ON bs.metric_fact_id = mf.id
GROUP BY mf.brand_id, mf.customer_id, mf.collector_type, DATE(mf.processed_at);

-- Unique index for fast lookups
CREATE UNIQUE INDEX idx_mv_brand_daily_metrics_lookup
  ON mv_brand_daily_metrics (brand_id, collector_type, metric_date);

-- Index for date range queries
CREATE INDEX idx_mv_brand_daily_metrics_date_range
  ON mv_brand_daily_metrics (brand_id, metric_date DESC);
```

#### 1.3 Add Comments

```sql
-- Documentation
COMMENT ON TABLE metric_facts IS 'Core fact table linking collector results to metrics';
COMMENT ON TABLE brand_metrics IS 'Brand-specific visibility and position metrics';
COMMENT ON TABLE competitor_metrics IS 'Competitor-specific visibility and position metrics';
COMMENT ON TABLE brand_sentiment IS 'Brand sentiment analysis results';
COMMENT ON TABLE competitor_sentiment IS 'Competitor sentiment analysis results';
COMMENT ON MATERIALIZED VIEW mv_brand_daily_metrics IS 'Pre-aggregated daily metrics for fast dashboard queries';
```

**‚úÖ Phase 1 Complete**: New tables created, ready for data

---

### Phase 2: Backfill Historical Data (Day 3-5)
**Duration**: 8-24 hours (background job)  
**Downtime**: ZERO  
**Risk**: üü¢ LOW (read-only operation on existing data)

#### 2.1 Create Migration Script

**File**: `backend/src/scripts/migrate-to-new-schema.ts`

```typescript
import { supabaseAdmin } from '../config/database';

async function migrateToNewSchema() {
  console.log('üöÄ Starting migration to new schema...\n');
  
  const BATCH_SIZE = 500;
  let offset = 0;
  let totalMigrated = 0;
  let errors = 0;
  
  while (true) {
    console.log(`üì¶ Processing batch starting at offset ${offset}...`);
    
    // Fetch batch of extracted_positions (brand rows only)
    const { data: brandRows, error: fetchError } = await supabaseAdmin
      .from('extracted_positions')
      .select('*')
      .is('competitor_name', null)
      .order('id', { ascending: true })
      .range(offset, offset + BATCH_SIZE - 1);
    
    if (fetchError) {
      console.error('‚ùå Error fetching batch:', fetchError);
      errors++;
      break;
    }
    
    if (!brandRows || brandRows.length === 0) {
      console.log('‚úÖ No more rows to process');
      break;
    }
    
    console.log(`   Found ${brandRows.length} brand rows in batch`);
    
    // Process each brand row
    for (const brandRow of brandRows) {
      try {
        await migrateSingleResult(brandRow);
        totalMigrated++;
        
        if (totalMigrated % 100 === 0) {
          console.log(`   ‚úÖ Migrated ${totalMigrated} results...`);
        }
      } catch (error) {
        console.error(`   ‚ùå Error migrating collector_result ${brandRow.collector_result_id}:`, error);
        errors++;
      }
    }
    
    offset += BATCH_SIZE;
    
    // Small delay to avoid overwhelming database
    await new Promise(resolve => setTimeout(resolve, 100));
  }
  
  console.log('\nüéâ Migration complete!');
  console.log(`   Total migrated: ${totalMigrated}`);
  console.log(`   Errors: ${errors}`);
  
  // Refresh materialized view
  console.log('\nüîÑ Refreshing materialized view...');
  await supabaseAdmin.rpc('refresh_mv_brand_daily_metrics');
  console.log('‚úÖ Materialized view refreshed');
}

async function migrateSingleResult(brandRow: any) {
  // Step 1: Insert metric_fact
  const { data: metricFact, error: factError } = await supabaseAdmin
    .from('metric_facts')
    .insert({
      collector_result_id: brandRow.collector_result_id,
      brand_id: brandRow.brand_id,
      customer_id: brandRow.customer_id,
      query_id: brandRow.query_id,
      collector_type: brandRow.collector_type,
      topic: brandRow.topic,
      processed_at: brandRow.processed_at,
      created_at: brandRow.created_at
    })
    .select()
    .single();
  
  if (factError) {
    // Check if already exists (idempotency)
    if (factError.code === '23505') { // Unique violation
      return; // Skip, already migrated
    }
    throw factError;
  }
  
  // Step 2: Insert brand_metrics
  await supabaseAdmin
    .from('brand_metrics')
    .insert({
      metric_fact_id: metricFact.id,
      visibility_index: brandRow.visibility_index,
      share_of_answers: brandRow.share_of_answers_brand,
      has_brand_presence: brandRow.has_brand_presence,
      brand_first_position: brandRow.brand_first_position,
      brand_positions: brandRow.brand_positions,
      total_brand_mentions: brandRow.total_brand_mentions,
      total_word_count: brandRow.total_word_count
    });
  
  // Step 3: Insert brand_sentiment (if exists)
  if (brandRow.sentiment_label || brandRow.sentiment_score) {
    await supabaseAdmin
      .from('brand_sentiment')
      .insert({
        metric_fact_id: metricFact.id,
        sentiment_label: brandRow.sentiment_label,
        sentiment_score: brandRow.sentiment_score,
        positive_sentences: brandRow.sentiment_positive_sentences || [],
        negative_sentences: brandRow.sentiment_negative_sentences || []
      });
  }
  
  // Step 4: Fetch and insert competitor rows
  const { data: competitorRows } = await supabaseAdmin
    .from('extracted_positions')
    .select('*')
    .eq('collector_result_id', brandRow.collector_result_id)
    .not('competitor_name', 'is', null);
  
  if (competitorRows && competitorRows.length > 0) {
    for (const compRow of competitorRows) {
      // Get competitor_id from brand_competitors table
      const { data: competitor } = await supabaseAdmin
        .from('brand_competitors')
        .select('id')
        .eq('brand_id', brandRow.brand_id)
        .eq('competitor_name', compRow.competitor_name)
        .single();
      
      if (competitor) {
        // Insert competitor_metrics
        await supabaseAdmin
          .from('competitor_metrics')
          .insert({
            metric_fact_id: metricFact.id,
            competitor_id: competitor.id,
            visibility_index: compRow.visibility_index_competitor,
            share_of_answers: compRow.share_of_answers_competitor,
            competitor_positions: compRow.competitor_positions,
            competitor_mentions: compRow.competitor_mentions
          });
        
        // Insert competitor_sentiment (if exists)
        if (compRow.sentiment_label_competitor || compRow.sentiment_score_competitor) {
          await supabaseAdmin
            .from('competitor_sentiment')
            .insert({
              metric_fact_id: metricFact.id,
              competitor_id: competitor.id,
              sentiment_label: compRow.sentiment_label_competitor,
              sentiment_score: compRow.sentiment_score_competitor,
              positive_sentences: compRow.sentiment_positive_sentences_competitor || [],
              negative_sentences: compRow.sentiment_negative_sentences_competitor || []
            });
        }
      }
    }
  }
}

migrateToNewSchema().catch(console.error);
```

#### 2.2 Run Migration

```bash
# Run migration script
cd /Users/vikas/Documents/evidently/backend
npx ts-node src/scripts/migrate-to-new-schema.ts
```

#### 2.3 Verify Migration

```sql
-- Check row counts
SELECT 'metric_facts' as table_name, COUNT(*) as count FROM metric_facts
UNION ALL
SELECT 'brand_metrics', COUNT(*) FROM brand_metrics
UNION ALL
SELECT 'competitor_metrics', COUNT(*) FROM competitor_metrics
UNION ALL
SELECT 'brand_sentiment', COUNT(*) FROM brand_sentiment
UNION ALL
SELECT 'competitor_sentiment', COUNT(*) FROM competitor_sentiment
UNION ALL
SELECT 'extracted_positions (brand)', COUNT(*) FROM extracted_positions WHERE competitor_name IS NULL;

-- Expected: metric_facts = brand_metrics = extracted_positions (brand)
```

**‚úÖ Phase 2 Complete**: Historical data migrated to new schema

---

### Phase 3: Implement Dual-Write (Day 6-8)
**Duration**: 4-8 hours  
**Downtime**: ZERO  
**Risk**: üü° MEDIUM (code changes, but old system still works)

#### 3.1 Update Scoring Service to Write to Both Tables

**File**: `backend/src/services/scoring/position-extraction.service.ts`

Add new method:

```typescript
/**
 * Save positions to NEW schema (metric_facts + domain tables)
 */
private async savePositionsToNewSchema(
  collectorResultId: number,
  brandRow: PositionInsertRow,
  competitorRows: PositionInsertRow[]
): Promise<void> {
  console.log(`   üíæ [NEW SCHEMA] Saving positions for collector_result ${collectorResultId}`);
  
  try {
    // Step 1: Insert metric_fact
    const { data: metricFact, error: factError } = await this.supabase
      .from('metric_facts')
      .insert({
        collector_result_id: collectorResultId,
        brand_id: brandRow.brand_id,
        customer_id: brandRow.customer_id,
        query_id: brandRow.query_id,
        collector_type: brandRow.collector_type,
        topic: brandRow.topic,
        processed_at: new Date().toISOString()
      })
      .select()
      .single();
    
    if (factError) {
      console.error(`   ‚ùå [NEW SCHEMA] Error creating metric_fact:`, factError);
      return; // Don't block if new schema fails
    }
    
    // Step 2: Insert brand_metrics
    await this.supabase
      .from('brand_metrics')
      .insert({
        metric_fact_id: metricFact.id,
        visibility_index: brandRow.visibility_index,
        share_of_answers: brandRow.share_of_answers_brand,
        has_brand_presence: brandRow.has_brand_presence,
        brand_first_position: brandRow.brand_first_position,
        brand_positions: brandRow.brand_positions,
        total_brand_mentions: brandRow.total_brand_mentions,
        total_word_count: brandRow.total_word_count
      });
    
    // Step 3: Insert brand_sentiment (if exists)
    if (brandRow.sentiment_label || brandRow.sentiment_score) {
      await this.supabase
        .from('brand_sentiment')
        .insert({
          metric_fact_id: metricFact.id,
          sentiment_label: brandRow.sentiment_label,
          sentiment_score: brandRow.sentiment_score,
          positive_sentences: brandRow.sentiment_positive_sentences || [],
          negative_sentences: brandRow.sentiment_negative_sentences || []
        });
    }
    
    // Step 4: Insert competitor_metrics and sentiment
    for (const compRow of competitorRows) {
      // Get competitor_id
      const { data: competitor } = await this.supabase
        .from('brand_competitors')
        .select('id')
        .eq('brand_id', brandRow.brand_id)
        .eq('competitor_name', compRow.competitor_name)
        .single();
      
      if (competitor) {
        // Insert competitor_metrics
        await this.supabase
          .from('competitor_metrics')
          .insert({
            metric_fact_id: metricFact.id,
            competitor_id: competitor.id,
            visibility_index: compRow.visibility_index_competitor,
            share_of_answers: compRow.share_of_answers_competitor,
            competitor_positions: compRow.competitor_positions,
            competitor_mentions: compRow.competitor_mentions
          });
        
        // Insert competitor_sentiment (if exists)
        if (compRow.sentiment_label_competitor || compRow.sentiment_score_competitor) {
          await this.supabase
            .from('competitor_sentiment')
            .insert({
              metric_fact_id: metricFact.id,
              competitor_id: competitor.id,
              sentiment_label: compRow.sentiment_label_competitor,
              sentiment_score: compRow.sentiment_score_competitor,
              positive_sentences: compRow.sentiment_positive_sentences_competitor || [],
              negative_sentences: compRow.sentiment_negative_sentences_competitor || []
            });
        }
      }
    }
    
    console.log(`   ‚úÖ [NEW SCHEMA] Successfully saved positions for collector_result ${collectorResultId}`);
  } catch (error) {
    console.error(`   ‚ö†Ô∏è [NEW SCHEMA] Error saving to new schema (non-blocking):`, error);
    // Don't throw - old schema write will still succeed
  }
}
```

Update existing `savePositions` method:

```typescript
private async savePositions(/* ... */) {
  // ... existing code to save to extracted_positions ...
  
  // DUAL-WRITE: Also save to new schema (Phase 3)
  await this.savePositionsToNewSchema(collectorResultId, brandRow, competitorRows);
}
```

#### 3.2 Test Dual-Write

```bash
# Run scoring on a small dataset
curl -X POST http://localhost:3000/api/admin/brands/:brandId/score-now \
  -H "Content-Type: application/json" \
  -d '{"customer_id": "...", "limit": 10}'

# Verify data appears in both old and new tables
```

**‚úÖ Phase 3 Complete**: System now writes to both old and new schemas

---

### Phase 4: Migrate Dashboard Queries (Day 9-14)
**Duration**: 1-2 weeks (gradual)  
**Downtime**: ZERO  
**Risk**: üü° MEDIUM (query changes, extensive testing needed)

This is the most critical phase. We'll migrate queries service by service.

#### 4.1 Create Query Helper Service

**File**: `backend/src/services/query-adapter.service.ts`

```typescript
/**
 * Query adapter to abstract old vs new schema
 * Allows gradual migration of queries
 */
export class QueryAdapterService {
  constructor(private supabase: SupabaseClient) {}
  
  /**
   * Get brand visibility metrics (7-day view)
   */
  async getBrandVisibilityMetrics(
    brandId: string,
    startDate: string,
    endDate: string,
    collectorTypes?: string[]
  ) {
    // USE NEW SCHEMA: Materialized view (90x faster!)
    let query = this.supabase
      .from('mv_brand_daily_metrics')
      .select('*')
      .eq('brand_id', brandId)
      .gte('metric_date', startDate)
      .lte('metric_date', endDate);
    
    if (collectorTypes && collectorTypes.length > 0) {
      query = query.in('collector_type', collectorTypes);
    }
    
    return await query;
  }
  
  /**
   * Get competitor visibility trend
   */
  async getCompetitorVisibilityTrend(
    brandId: string,
    competitorName: string,
    startDate: string,
    endDate: string
  ) {
    // USE NEW SCHEMA: Normalized competitor_metrics (10x faster!)
    const { data: competitor } = await this.supabase
      .from('brand_competitors')
      .select('id')
      .eq('brand_id', brandId)
      .eq('competitor_name', competitorName)
      .single();
    
    if (!competitor) {
      return { data: [], error: null };
    }
    
    const { data, error } = await this.supabase
      .from('competitor_metrics')
      .select(`
        metric_fact_id,
        visibility_index,
        share_of_answers,
        metric_facts!inner(processed_at, collector_type)
      `)
      .eq('competitor_id', competitor.id)
      .gte('metric_facts.processed_at', startDate)
      .lte('metric_facts.processed_at', endDate);
    
    return { data, error };
  }
}
```

#### 4.2 Migrate Dashboard Service (Highest Priority)

**File**: `backend/src/services/brand-dashboard/payload-builder.ts`

**Current (OLD SCHEMA)**:
```typescript
const { data: positions } = await supabaseAdmin
  .from('extracted_positions')
  .select('*')
  .eq('brand_id', brand.id)
  .is('competitor_name', null)
  .gte('processed_at', startDate);
// ... complex aggregation in application code ...
```

**New (NEW SCHEMA with Materialized View)**:
```typescript
// Much simpler! Pre-aggregated data
const { data: dailyMetrics } = await supabaseAdmin
  .from('mv_brand_daily_metrics')
  .select('*')
  .eq('brand_id', brand.id)
  .gte('metric_date', startDate);

// Data is already aggregated by day + collector!
// No need for complex GROUP BY in application
```

**Migration Steps**:

1. **Test new query on staging** (verify results match)
2. **Add feature flag** to toggle between old/new
3. **Deploy to production** with flag=OFF
4. **Enable flag for 10% of users** (canary deployment)
5. **Monitor performance** and errors
6. **Gradually increase** to 50%, then 100%
7. **Remove old query code** once stable

#### 4.3 Service-by-Service Migration Priority

| Service | Priority | Complexity | Est. Time |
|---------|----------|------------|-----------|
| **Dashboard** | üî¥ Highest | Medium | 2-3 days |
| **Visibility Charts** | üî¥ High | Low | 1 day |
| **Topics Page** | üü° Medium | Medium | 2 days |
| **Keywords Analytics** | üü° Medium | Medium | 2 days |
| **Prompts Analytics** | üü¢ Low | Low | 1 day |
| **Source Attribution** | üü¢ Low | High | 3 days |

**‚úÖ Phase 4 Complete**: All queries migrated to new schema

---

### Phase 5: Deprecate Old Table (Day 15-21)
**Duration**: 1 week (validation + cleanup)  
**Downtime**: ZERO  
**Risk**: üü¢ LOW (old table kept as backup)

#### 5.1 Stop Writing to Old Table

```typescript
// Remove dual-write code from position-extraction.service.ts
private async savePositions(/* ... */) {
  // Only write to new schema
  await this.savePositionsToNewSchema(collectorResultId, brandRow, competitorRows);
  
  // OLD: Remove this
  // await this.supabase.from('extracted_positions').insert([...]);
}
```

#### 5.2 Archive Old Table (Don't Drop Yet!)

```sql
-- Rename table to indicate it's archived
ALTER TABLE extracted_positions RENAME TO extracted_positions_archived;

-- Remove indexes to save space (but keep data)
DROP INDEX IF EXISTS idx_extracted_positions_brand_id;
DROP INDEX IF EXISTS idx_extracted_positions_collector_result_id;
-- ... drop all indexes ...
```

#### 5.3 Final Validation

Run for 1 week with new schema only:
- Monitor error logs
- Check query performance
- Verify data accuracy
- User acceptance testing

#### 5.4 Final Cleanup (After 1 Month)

```sql
-- Only after confirming everything works for 1 month
DROP TABLE IF EXISTS extracted_positions_archived;

-- Remove raw_answer from collector_results (if desired)
ALTER TABLE collector_results DROP COLUMN raw_answer;
```

**‚úÖ Phase 5 Complete**: Migration finished, old table archived

---

## üìä Success Metrics

### Performance Improvements

| Metric | Before | After | Target |
|--------|--------|-------|--------|
| **Dashboard Load Time** | 2-3s | 200-300ms | ‚úÖ 90% faster |
| **Storage** | 1,840MB | 294MB | ‚úÖ 84% reduction |
| **Index Size** | 450MB | 150MB | ‚úÖ 67% reduction |
| **Write Time** | 500ms | 100ms | ‚úÖ 80% faster |

### Monitoring Dashboard

Create monitoring queries:

```sql
-- Query performance comparison
SELECT 
  'OLD: Brand-only query' as query_type,
  COUNT(*) as row_count,
  pg_size_pretty(pg_total_relation_size('extracted_positions_archived')) as table_size
FROM extracted_positions_archived
WHERE competitor_name IS NULL

UNION ALL

SELECT 
  'NEW: Brand metrics query',
  COUNT(*),
  pg_size_pretty(pg_total_relation_size('brand_metrics'))
FROM brand_metrics;
```

---

## üö® Rollback Plans

### If Phase 3 Fails (Dual-Write Issues)

```typescript
// Disable new schema writes
const ENABLE_NEW_SCHEMA = false; // Feature flag

if (ENABLE_NEW_SCHEMA) {
  await this.savePositionsToNewSchema(/* ... */);
}

// Old schema continues working normally
```

### If Phase 4 Fails (Query Issues)

```typescript
// Feature flag per service
const USE_NEW_SCHEMA = {
  dashboard: false,  // Rollback if issues
  topics: true,      // Keep if working
  keywords: true
};

if (USE_NEW_SCHEMA.dashboard) {
  // Use new schema
} else {
  // Fallback to old schema
}
```

### Emergency Rollback

If critical issues occur:

1. **Stop new writes**: Set `ENABLE_NEW_SCHEMA = false`
2. **Revert queries**: Use old schema for reads
3. **Investigate**: Check logs, analyze errors
4. **Fix issues**: Update code, test thoroughly
5. **Retry**: Re-enable new schema gradually

**Old data is never deleted until Phase 5 validation complete**

---

## ‚úÖ Approval Checklist

Before starting implementation, confirm:

- [ ] **Understand the plan**: 5 phases, gradual migration, zero downtime
- [ ] **Resource availability**: Database space for dual-storage (temp increase)
- [ ] **Backup strategy**: Full database backup before Phase 1
- [ ] **Testing environment**: Staging database to test each phase
- [ ] **Monitoring**: Error tracking, performance monitoring in place
- [ ] **Rollback plan**: Understand how to revert if issues occur
- [ ] **Timeline**: 2-3 weeks for full migration acceptable

---

## üéØ Next Steps (Once Approved)

1. **Review this plan** - Ask questions, clarify any concerns
2. **Approve plan** - Confirm you're ready to proceed
3. **I'll implement Phase 1** - Create new schema (2-4 hours, zero risk)
4. **You test Phase 1** - Verify tables created correctly
5. **I'll implement Phase 2** - Backfill data (8-24 hours, background)
6. **Continue phase by phase** - With testing after each phase

**Ready to proceed?** Let me know if you:
- ‚úÖ **Approve** - I'll start with Phase 1
- ‚ùì **Have questions** - I'll clarify anything
- üîÑ **Want modifications** - I'll adjust the plan

